[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feature Attribution for tree ensembles",
    "section": "",
    "text": "For a dataset with \\(p\\) features, each prediction on the dataset is decomposed as prediction = bias + feature_1_contribution + ... + feature_p_contribution. From hereon we refer to these “contributions” as Conditional Feature Contributions (CFCs).\nCFCs provide local, case-specific explanations for predictions by tracing the decision path and assigning changes in the model’s expected output to individual features along that path. However, CFCs are prone to bias, which increases with distance from the root of the decision tree. An increasingly popular alternative, SHapley Additive exPlanation (SHAP) values, seem to reduce this bias but come with a significantly higher computational cost.\n(Loecher, Lai, and Qi 2022) present an empirical comparison of the explanations produced by both methods, using a dataset of 164 publicly available classification problems. For random forests and boosted trees, they observe remarkably high correlations and similarities between local and global SHAP values and CFC scores, resulting in comparable rankings and interpretations. These findings also extend to global feature importance scores, reinforcing their use as proxies for the predictive power of individual features.\n\n“We generated several thousand feature subsets for each dataset, re-trained models for each subset, and then measured the correlation between the model loss and the total importance of the included features”. In particular, the feature subsets were independently sampled as follows: sample the subset’s cardinality k ∈ 0, 1,.., d uniformly at random, then select k elements from 1,.., d uniformly at random. We computed both SHAP/CFC and the retrained models’ loss (either negative log loss or rmse, respectively) on a test set. This strategy yields one correlation coefficient for SHAP and CFC for each of the data sets described in Sect. 2.1.”\n\nThe following figure depicts these correlations for random forests and boosted trees as a scatterplot with marginal distributions\n\n\n\n\n\n\nLoecher, Markus, Dingyi Lai, and Wu Qi. 2022. “Approximation of SHAP Values for Randomized Tree Ensembles.” In International Cross-Domain Conference for Machine Learning and Knowledge Extraction, 19–30. Springer."
  },
  {
    "objectID": "CFC_SHAP_Visualization.html",
    "href": "CFC_SHAP_Visualization.html",
    "title": "2  SHAP Plots with treeinterpreter",
    "section": "",
    "text": "In this notebook, we’ll demonstrate how to create SHAP-type plots using the treeinterpreter package along with SHAP. SHAP (SHapley Additive exPlanations) plots are a popular method for interpreting machine learning models by showing the contribution of each feature to a specific prediction. treeinterpreter is a tool that breaks down the predictions of tree-based models (e.g., Random Forests) into individual feature contributions. By combining these two tools, we can visualize and interpret the impact of features on model predictions."
  },
  {
    "objectID": "CFC_SHAP_Visualization.html#prerequisites",
    "href": "CFC_SHAP_Visualization.html#prerequisites",
    "title": "2  SHAP Plots with treeinterpreter",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore running the examples, ensure that you have the following libraries installed: - shap - treeinterpreter - scikit-learn\nYou can install them using:\npip install shap treeinterpreter scikit-learn"
  },
  {
    "objectID": "CFC_SHAP_Visualization.html#table-of-contents",
    "href": "CFC_SHAP_Visualization.html#table-of-contents",
    "title": "2  SHAP Plots with treeinterpreter",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nExample 1: Iris Dataset\nExample 2: Wine Dataset\n\n\n#!pip install shap treeinterpreter\n\n\nimport shap\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.datasets import load_iris\nfrom treeinterpreter import treeinterpreter as ti\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "CFC_SHAP_Visualization.html#example-1-iris-dataset",
    "href": "CFC_SHAP_Visualization.html#example-1-iris-dataset",
    "title": "2  SHAP Plots with treeinterpreter",
    "section": "Example 1: Iris Dataset",
    "text": "Example 1: Iris Dataset\nThis script loads the Iris dataset, trains a RandomForestClassifier model, and uses TreeInterpreter and SHAP to analyze feature contributions.\n\nSteps:\n\nLoad the Iris dataset and train a RandomForestClassifier model.\nUse TreeInterpreter to obtain predictions, biases, and feature contributions.\nFocus on contributions for one class (class 0 in this example).\nCreate a SHAP Explanation object.\nGenerate various SHAP plots including a summary plot, a waterfall plot for the first instance, and a bar plot of the mean absolute SHAP values across all features.\n\n\nimport numpy as np\nimport shap\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom treeinterpreter import treeinterpreter as ti\nimport matplotlib.pyplot as plt\n\n# Load dataset and train the model\ndata = load_iris()\nX, y = data.data, data.target\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Use treeinterpreter to get the prediction, bias, and contributions\nprediction, bias, contributions = ti.predict(model, X)\n\n# Contributions.shape is (n_samples, n_features, n_classes)\n# We reduce the dimensionality by selecting one class\nshap_values = contributions[:, :, 0]  # Choose class 0 for visualization\n\n# Creating a SHAP Explanation object\nexplainer = shap.Explainer(model)\nshap_object = shap.Explanation(\n    values=shap_values,\n    base_values=bias[:, 0],  # Base values should match the selected class\n    data=X,\n    feature_names=data.feature_names\n)\n\n# Generate SHAP summary plot (beeswarm plot) and modify the x-axis label directly\nshap.summary_plot(shap_object.values, shap_object.data, feature_names=shap_object.feature_names, show=False)\nplt.gca().set_xlabel(\"CFC values\")  # Modify the x-axis label\nplt.show()  # Display the plot with the updated label\n\n\n\n\nGenerate SHAP waterfall plot for the first instance\n\nshap.waterfall_plot(shap_object[0])\n\n\n\n\nFor global importances, average the mean absolute values across all instances\n\nmean_abs_shap_values = np.abs(shap_object.values).mean(axis=0)\nshap.bar_plot(mean_abs_shap_values, feature_names=shap_object.feature_names)"
  },
  {
    "objectID": "CFC_SHAP_Visualization.html#example-2-wine-dataset",
    "href": "CFC_SHAP_Visualization.html#example-2-wine-dataset",
    "title": "2  SHAP Plots with treeinterpreter",
    "section": "Example 2: Wine Dataset",
    "text": "Example 2: Wine Dataset\nThis script demonstrates the process of loading the Wine dataset, training a RandomForestClassifier model, and using TreeInterpreter and SHAP to analyze feature contributions. It includes generating several SHAP plots for visualizing the contributions.\n\nSteps:\n\nLoad the Wine dataset and train a RandomForestClassifier model.\nUse TreeInterpreter to obtain predictions, biases, and feature contributions.\nFocus on contributions for one class (class 0 in this example).\nCreate a SHAP Explanation object.\nGenerate SHAP plots including a summary plot, a waterfall plot for the first instance, and a bar plot showing the mean absolute SHAP values across all features.\n\n\nimport numpy as np\nimport shap\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import RandomForestClassifier\nfrom treeinterpreter import treeinterpreter as ti\nimport matplotlib.pyplot as plt\n\n# Load the Wine dataset and train the model\ndata = load_wine()\nX, y = data.data, data.target\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Use treeinterpreter to get the prediction, bias, and contributions\nprediction, bias, contributions = ti.predict(model, X)\n\n# contributions.shape is (n_samples, n_features, n_classes)\n# We reduce the dimensionality by selecting one class\nshap_values = contributions[:, :, 0]  # Choose class 0 for visualization\n\n# Creating a SHAP Explanation object\nexplainer = shap.Explainer(model)\nshap_object = shap.Explanation(\n    values=shap_values,\n    base_values=bias[:, 0],  # Base values should match the selected class\n    data=X,\n    feature_names=data.feature_names\n)\n\n# Generate SHAP summary plot (beeswarm plot) and modify the x-axis label directly\nshap.summary_plot(shap_object.values, shap_object.data, feature_names=shap_object.feature_names, show=False)\nplt.gca().set_xlabel(\"CFC values\")  # Modify the x-axis label\nplt.show()  # Display the plot with the updated label\n\n\n\n\nGenerate SHAP waterfall plot for the first instance\n\nshap.waterfall_plot(shap_object[0])\n\n\n\n\nFor global importances, average the mean absolute values across all instances\n\nmean_abs_shap_values = np.abs(shap_object.values).mean(axis=0)\nshap.bar_plot(mean_abs_shap_values, feature_names=shap_object.feature_names)\n\n\n\n\n\nReferences"
  }
]